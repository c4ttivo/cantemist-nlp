{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk matplotlib pandas textstat fonemas"
      ],
      "metadata": {
        "id": "88a5pd4YIoTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar solo una vez\n",
        "nltk.download('stopwords')\n",
        "stopwords_esp = set(stopwords.words('spanish'))\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from fonemas import Transcription\n",
        "import textstat\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from IPython.display import display\n"
      ],
      "metadata": {
        "id": "ixeHXKESGubu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento inicial"
      ],
      "metadata": {
        "id": "bchL6OJjHuOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def descomprimir_zip(ruta_zip, carpeta_destino):\n",
        "    # Verificar si el archivo existe\n",
        "    if not os.path.exists(ruta_zip):\n",
        "        print(f\"El archivo {ruta_zip} no existe.\")\n",
        "        return\n",
        "\n",
        "    # Crear carpeta destino si no existe\n",
        "    if not os.path.exists(carpeta_destino):\n",
        "        os.makedirs(carpeta_destino)\n",
        "\n",
        "    # Abrir y extraer el zip\n",
        "    with zipfile.ZipFile(ruta_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(carpeta_destino)\n",
        "        print(f\"Archivo descomprimido en: {carpeta_destino}\")\n",
        "\n",
        "# Ejemplo de uso\n",
        "ruta_zip = \"cantemist.zip\"            # Ruta al archivo zip\n",
        "carpeta_destino = \"cantemist\"  # Carpeta donde se extraerá\n",
        "descomprimir_zip(ruta_zip, carpeta_destino)\n"
      ],
      "metadata": {
        "id": "8gWSC0_yHpGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def cargar_textos_carpeta(carpeta_textos):\n",
        "    documentos = []\n",
        "    for archivo in os.listdir(carpeta_textos):\n",
        "        if archivo.endswith(\".txt\"):\n",
        "            ruta = os.path.join(carpeta_textos, archivo)\n",
        "            with open(ruta, \"r\", encoding=\"utf-8\") as f:\n",
        "                contenido = f.read()\n",
        "                documentos.append({\"archivo\": archivo, \"texto\": contenido})\n",
        "    return pd.DataFrame(documentos)\n",
        "\n",
        "# Ruta a la carpeta de entrenamiento (ajusta según tu estructura)\n",
        "carpeta_train = \"/content/cantemist/background-set\"\n",
        "\n",
        "df_textos = cargar_textos_carpeta(carpeta_train)\n",
        "print(df_textos.shape)\n",
        "# Muestra los primeros textos\n",
        "df_textos#.head()\n"
      ],
      "metadata": {
        "id": "9FnGsaRFMxH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Contar palabras por archivo\n",
        "df_textos[\"word_count\"] = df_textos[\"texto\"].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Mostrar cada archivo con su cantidad de palabras\n",
        "print(df_textos[[\"archivo\", \"word_count\"]])\n",
        "\n",
        "# Calcular promedio de palabras por archivo\n",
        "promedio_palabras = df_textos[\"word_count\"].mean()\n",
        "print(\"\\nNúmero promedio de palabras por archivo:\", promedio_palabras)\n"
      ],
      "metadata": {
        "id": "W5vJL4W3RhYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenar_textos(carpeta_textos):\n",
        "    corpus = []\n",
        "    for archivo in os.listdir(carpeta_textos):\n",
        "        if archivo.endswith(\".txt\"):\n",
        "            ruta = os.path.join(carpeta_textos, archivo)\n",
        "            with open(ruta, \"r\", encoding=\"utf-8\") as f:\n",
        "                contenido = f.read()\n",
        "                corpus.append(contenido)\n",
        "    return \" \".join(corpus)\n",
        "\n",
        "# Ruta a la carpeta de entrenamiento (ajústala a tu entorno)\n",
        "carpeta_train = \"/content/cantemist/background-set\"\n",
        "\n",
        "corpus_general = concatenar_textos(carpeta_train)\n",
        "\n",
        "print(\"Longitud total del corpus (caracteres):\", len(corpus_general))\n",
        "print(\"Primeros 500 caracteres:\\n\", corpus_general[:500])"
      ],
      "metadata": {
        "id": "U2vQGm78NiTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceZyVMWxGA6M"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calcular_textometria(texto):\n",
        "    return {\n",
        "        \"Flesch Reading Ease\": textstat.flesch_reading_ease(texto),\n",
        "        \"Gunning Fog Index\": textstat.gunning_fog(texto),\n",
        "        \"Automated Readability Index\": textstat.automated_readability_index(texto),\n",
        "        \"SMOG Index\": textstat.smog_index(texto),\n",
        "        \"Dale-Chall Readability Score\": textstat.dale_chall_readability_score(texto),\n",
        "        \"Fernández-Huerta\": textstat.fernandez_huerta(texto),\n",
        "        \"Gutierrez-Polini\": textstat.gutierrez_polini(texto),\n",
        "        \"Szigriszt-Pazos\": textstat.szigriszt_pazos(texto),\n",
        "        \"Crawford\": textstat.crawford(texto),\n",
        "        \"Word Count\": textstat.lexicon_count(texto, removepunct=True)  # número de palabras\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calcular_textometria(corpus_general)"
      ],
      "metadata": {
        "id": "2E3M28WKIeuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesar_texto(texto, remover_stopwords=True):\n",
        "    \"\"\"\n",
        "    Preprocesa un texto:\n",
        "      - Convierte a minúsculas\n",
        "      - Elimina caracteres no alfanuméricos\n",
        "      - Tokeniza en palabras\n",
        "      - (Opcional) elimina stopwords en español\n",
        "    \"\"\"\n",
        "    # 1. Minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # 2. Eliminar caracteres que no sean letras o números\n",
        "    texto = re.sub(r'[^a-záéíóúüñ0-9\\s]', ' ', texto)\n",
        "\n",
        "    # 3. Tokenizar (separar por espacios)\n",
        "    tokens = texto.split()\n",
        "\n",
        "    # 4. Remover stopwords si se activa\n",
        "    if remover_stopwords:\n",
        "        tokens = [t for t in tokens if t not in stopwords_esp]\n",
        "\n",
        "    # 5. Reconstruir texto limpio\n",
        "    texto_limpio = \" \".join(tokens)\n",
        "\n",
        "    return texto_limpio"
      ],
      "metadata": {
        "id": "zTgk0Vm-PiUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCIÓN: Análisis Léxico - Ley de Zipf\n",
        "def analizar_ley_zipf(texto, titulo):\n",
        "    tokens = word_tokenize(texto.lower())\n",
        "    tokens = [t for t in tokens if re.match(r'^\\w+$', t)]  # Solo palabras alfanuméricas\n",
        "    #tokens = [t for t in tokens if t not in stopwords_esp]\n",
        "    freqs = Counter(tokens)\n",
        "    df = pd.DataFrame(freqs.items(), columns=['Palabra', 'Frecuencia'])\n",
        "    df = df.sort_values(by='Frecuencia', ascending=False).reset_index(drop=True)\n",
        "    df['Rango'] = df.index + 1\n",
        "    df['Ley_Zipf'] = df['Frecuencia'].iloc[0] / (df['Rango'])\n",
        "    print(\"De search: \",df[df[\"Palabra\"]==\"de\"])\n",
        "    print()\n",
        "    print(df.head(20))\n",
        "    print()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df['Rango'], df['Frecuencia'], marker='o', label='Datos reales')\n",
        "    plt.plot(df['Rango'], df['Ley_Zipf'], linestyle='--', label='Zipf teórica')\n",
        "    plt.title(f\"Ley de Zipf - {titulo}\")\n",
        "    plt.xlabel(\"Rango\")\n",
        "    plt.ylabel(\"Frecuencia\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "# EJECUTAR ANÁLISIS LÉXICO\n",
        "df_zipf_true = analizar_ley_zipf(preprocesar_texto(texto = corpus_general, remover_stopwords=True), \"Ley de Zipf (Sin Stopword)\")\n",
        "df_zipf_false = analizar_ley_zipf(preprocesar_texto(texto = corpus_general, remover_stopwords=False), \"Ley de Zipf (Con Stopword)\")\n"
      ],
      "metadata": {
        "id": "OzmWfqo3N9cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus_general[:500])\n",
        "print(40*\"--\")\n",
        "preprocesar_texto(texto = corpus_general, remover_stopwords=False)[:500]"
      ],
      "metadata": {
        "id": "RHwsyn2-UEcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "\n",
        "# Asegúrate de tener stopwords descargadas\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def graficos_corpus(corpus_general, top_n=20, remover_stopwords=True):\n",
        "    # Tokenizar rápido\n",
        "    tokens = corpus_general.lower().split()\n",
        "\n",
        "    # Remover stopwords si se activa\n",
        "    if remover_stopwords:\n",
        "        stop_es = set(stopwords.words(\"spanish\"))\n",
        "        tokens = [t for t in tokens if t not in stop_es]\n",
        "\n",
        "    # --- Gráfico de barras ---\n",
        "    contador = Counter(tokens)\n",
        "    palabras_comunes = contador.most_common(top_n)\n",
        "\n",
        "    palabras, freqs = zip(*palabras_comunes)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.bar(palabras, freqs)\n",
        "    plt.grid()\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title(f\"Top {top_n} palabras más frecuentes\")\n",
        "    plt.ylabel(\"Frecuencia\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\")\n",
        "    # --- Nube de palabras ---\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\",\n",
        "                          max_words=200).generate(\" \".join(tokens))\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Nube de Palabras del Corpus\")\n",
        "    plt.show()\n",
        "\n",
        "# Ejemplo de uso:\n",
        "print(\"Remover Stopwords\")\n",
        "graficos_corpus(preprocesar_texto(texto = corpus_general, remover_stopwords=True), top_n=20, remover_stopwords=True)\n",
        "print(\"Conservar Stopwords\")\n",
        "graficos_corpus(preprocesar_texto(texto = corpus_general, remover_stopwords=False), top_n=20, remover_stopwords=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "XuThuvwxOwyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conteo_doc = df_textos.groupby(\"archivo\")[\"word_count\"].sum()\n",
        "conteo_doc"
      ],
      "metadata": {
        "id": "eBB1bQi2htfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.hist(df_textos[\"word_count\"], bins=20, edgecolor=\"black\")\n",
        "plt.xticks(range(0, max(conteo_doc)+250, 250))\n",
        "plt.xlabel(\"Número total de palabras en documento\")\n",
        "plt.ylabel(\"Cantidad de documentos\")\n",
        "plt.grid()\n",
        "plt.title(\"Histograma de palabras por documento\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PkmiBd0YR8D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pddSMd23hnjc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}