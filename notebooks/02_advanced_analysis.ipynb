{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrega 2 - Proyecto CANTEMIST\n",
    "# Análisis Avanzado de Corpus Biomédico con NER Expandido\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Instalación de librerías necesarias\n",
    "!pip install -q nltk matplotlib pandas textstat spacy scikit-learn wordcloud seaborn transformers datasets\n",
    "!pip install -q es_core_news_sm @https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl\n",
    "!python -m spacy download es_core_news_md\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "import textstat\n",
    "\n",
    "# Descargas NLTK necesarias\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Configuración de estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9c985",
   "metadata": {},
   "source": [
    "# ## 1. Correcciones y Modificaciones respecto a la Entrega 1\n",
    "# \n",
    "# ### Cambios implementados:\n",
    "# 1. **Tratamiento inteligente de stopwords**: Creación de lista personalizada para dominio médico\n",
    "# 2. **NER expandido**: Incorporación de múltiples categorías de entidades\n",
    "# 3. **Análisis semántico profundo**: Integración de embeddings y análisis contextual\n",
    "# 4. **Métricas avanzadas**: Diversidad léxica, coocurrencias, n-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e99a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo de spaCy para español\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# Función mejorada para cargar textos\n",
    "def cargar_textos_mejorado(carpeta_textos):\n",
    "    \"\"\"Carga textos con metadatos adicionales\"\"\"\n",
    "    documentos = []\n",
    "    for archivo in os.listdir(carpeta_textos):\n",
    "        if archivo.endswith(\".txt\"):\n",
    "            ruta = os.path.join(carpeta_textos, archivo)\n",
    "            with open(ruta, \"r\", encoding=\"utf-8\") as f:\n",
    "                contenido = f.read()\n",
    "                # Extraer ID del archivo\n",
    "                doc_id = archivo.replace('.txt', '')\n",
    "                documentos.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"archivo\": archivo, \n",
    "                    \"texto\": contenido,\n",
    "                    \"longitud_caracteres\": len(contenido),\n",
    "                    \"fecha_carga\": pd.Timestamp.now()\n",
    "                })\n",
    "    return pd.DataFrame(documentos)\n",
    "\n",
    "# Cargar datos\n",
    "carpeta_train = \"/content/cantemist/background-set\"\n",
    "df_textos = cargar_textos_mejorado(carpeta_train)\n",
    "print(f\"Total de documentos cargados: {len(df_textos)}\")\n",
    "print(f\"Columnas del dataset: {df_textos.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca644b2",
   "metadata": {},
   "source": [
    "# ### Creación de Stopwords Personalizadas para Dominio Médico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81664f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords personalizadas: conservar términos médicos importantes\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_base = set(stopwords.words('spanish'))\n",
    "\n",
    "# Términos médicos que NO deben ser eliminados (tienen valor clínico)\n",
    "terminos_medicos_importantes = {\n",
    "    'no', 'sin', 'con', 'anti', 'pre', 'post', 'contra', 'sobre',\n",
    "    'bajo', 'tras', 'ante', 'entre', 'dentro', 'fuera', 'después',\n",
    "    'antes', 'durante', 'mediante', 'versus', 'via', 'oral', 'total',\n",
    "    'parcial', 'agudo', 'crónico', 'primario', 'secundario'\n",
    "}\n",
    "\n",
    "# Crear stopwords personalizadas (removiendo términos médicos importantes)\n",
    "stopwords_medicas = stopwords_base - terminos_medicos_importantes\n",
    "\n",
    "print(f\"Stopwords originales: {len(stopwords_base)}\")\n",
    "print(f\"Stopwords personalizadas (dominio médico): {len(stopwords_medicas)}\")\n",
    "print(f\"Términos médicos conservados: {terminos_medicos_importantes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e6273",
   "metadata": {},
   "source": [
    "# ## 2. Análisis Exploratorio Avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7c37ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisis_exploratorio_avanzado(df):\n",
    "    \"\"\"Análisis exhaustivo del corpus\"\"\"\n",
    "    \n",
    "    # Estadísticas básicas\n",
    "    df['num_palabras'] = df['texto'].apply(lambda x: len(x.split()))\n",
    "    df['num_oraciones'] = df['texto'].apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "    df['num_caracteres'] = df['texto'].apply(len)\n",
    "    df['palabras_unicas'] = df['texto'].apply(lambda x: len(set(x.lower().split())))\n",
    "    \n",
    "    # Métricas de diversidad léxica\n",
    "    df['diversidad_lexica'] = df['palabras_unicas'] / df['num_palabras']\n",
    "    df['promedio_palabras_oracion'] = df['num_palabras'] / df['num_oraciones']\n",
    "    \n",
    "    # Análisis estadístico\n",
    "    stats_df = pd.DataFrame({\n",
    "        'Métrica': ['Palabras totales', 'Oraciones totales', 'Palabras únicas totales',\n",
    "                   'Promedio palabras/doc', 'Promedio oraciones/doc', \n",
    "                   'Diversidad léxica promedio'],\n",
    "        'Valor': [\n",
    "            df['num_palabras'].sum(),\n",
    "            df['num_oraciones'].sum(),\n",
    "            len(set(' '.join(df['texto']).lower().split())),\n",
    "            df['num_palabras'].mean(),\n",
    "            df['num_oraciones'].mean(),\n",
    "            df['diversidad_lexica'].mean()\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return df, stats_df\n",
    "\n",
    "df_textos, estadisticas = analisis_exploratorio_avanzado(df_textos)\n",
    "print(\"\\n=== ESTADÍSTICAS DEL CORPUS ===\")\n",
    "print(estadisticas.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96ab73",
   "metadata": {},
   "source": [
    "# ### Análisis de N-gramas y Coocurrencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641629db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "def extraer_ngramas(texto, n=2, num_top=20):\n",
    "    \"\"\"Extrae los n-gramas más frecuentes\"\"\"\n",
    "    # Tokenizar y limpiar\n",
    "    tokens = texto.lower().split()\n",
    "    tokens = [t for t in tokens if t not in stopwords_medicas and len(t) > 2]\n",
    "    \n",
    "    # Generar n-gramas\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    freq_dist = nltk.FreqDist(n_grams)\n",
    "    \n",
    "    return freq_dist.most_common(num_top)\n",
    "\n",
    "# Análisis de bigramas y trigramas\n",
    "corpus_completo = ' '.join(df_textos['texto'])\n",
    "\n",
    "print(\"\\n=== TOP 20 BIGRAMAS ===\")\n",
    "bigramas = extraer_ngramas(corpus_completo, n=2, num_top=20)\n",
    "for bigrama, freq in bigramas:\n",
    "    print(f\"{' '.join(bigrama)}: {freq}\")\n",
    "\n",
    "print(\"\\n=== TOP 20 TRIGRAMAS ===\")\n",
    "trigramas = extraer_ngramas(corpus_completo, n=3, num_top=20)\n",
    "for trigrama, freq in trigramas:\n",
    "    print(f\"{' '.join(trigrama)}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd19519",
   "metadata": {},
   "source": [
    "# ### Análisis de Colocaciones con Medidas de Asociación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a026438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_colocaciones(texto):\n",
    "    \"\"\"Encuentra colocaciones significativas usando PMI\"\"\"\n",
    "    tokens = texto.lower().split()\n",
    "    tokens = [t for t in tokens if t not in stopwords_medicas and len(t) > 2]\n",
    "    \n",
    "    # Bigramas con PMI\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(5)  # Mínimo 5 apariciones\n",
    "    \n",
    "    # Top colocaciones por PMI\n",
    "    pmi_bigramas = finder.nbest(bigram_measures.pmi, 15)\n",
    "    \n",
    "    # Trigramas con PMI\n",
    "    trigram_measures = TrigramAssocMeasures()\n",
    "    finder3 = TrigramCollocationFinder.from_words(tokens)\n",
    "    finder3.apply_freq_filter(3)\n",
    "    pmi_trigramas = finder3.nbest(trigram_measures.pmi, 10)\n",
    "    \n",
    "    return pmi_bigramas, pmi_trigramas\n",
    "\n",
    "bi_coloc, tri_coloc = analizar_colocaciones(corpus_completo)\n",
    "\n",
    "print(\"\\n=== COLOCACIONES SIGNIFICATIVAS (PMI) ===\")\n",
    "print(\"\\nBigramas:\")\n",
    "for bigrama in bi_coloc:\n",
    "    print(f\"  {' '.join(bigrama)}\")\n",
    "\n",
    "print(\"\\nTrigramas:\")\n",
    "for trigrama in tri_coloc:\n",
    "    print(f\"  {' '.join(trigrama)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d66414",
   "metadata": {},
   "source": [
    "# ## 3. NER Expandido - Múltiples Categorías de Entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f70ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_medico_expandido(texto):\n",
    "    \"\"\"\n",
    "    Reconocimiento de entidades expandido para dominio médico\n",
    "    Categorías: MORFOLOGÍA, PROCEDIMIENTO, MEDICAMENTO, SÍNTOMA, ANATOMÍA\n",
    "    \"\"\"\n",
    "    doc = nlp(texto[:1000000])  # Limitar por memoria\n",
    "    \n",
    "    entidades = {\n",
    "        'MORFOLOGIA': [],\n",
    "        'PROCEDIMIENTO': [],\n",
    "        'MEDICAMENTO': [],\n",
    "        'SINTOMA': [],\n",
    "        'ANATOMIA': [],\n",
    "        'ORGANIZACION': [],\n",
    "        'PERSONA': []\n",
    "    }\n",
    "    \n",
    "    # Patrones médicos específicos\n",
    "    patrones_medicos = {\n",
    "        'MORFOLOGIA': r'\\b(tumor|neoplasia|carcinoma|adenocarcinoma|metástasis|lesión|nódulo)\\b',\n",
    "        'PROCEDIMIENTO': r'\\b(biopsia|cirugía|radioterapia|quimioterapia|resonancia|tomografía|análisis)\\b',\n",
    "        'MEDICAMENTO': r'\\b(mg|ml|dosis|tratamiento|fármaco|medicamento|terapia)\\b',\n",
    "        'SINTOMA': r'\\b(dolor|fiebre|náuseas|vómito|fatiga|pérdida|aumento)\\b',\n",
    "        'ANATOMIA': r'\\b(mama|pulmón|hígado|riñón|cerebro|hueso|sangre|tejido|órgano)\\b'\n",
    "    }\n",
    "    \n",
    "    # Buscar patrones específicos\n",
    "    for categoria, patron in patrones_medicos.items():\n",
    "        matches = re.finditer(patron, texto.lower())\n",
    "        for match in matches:\n",
    "            entidades[categoria].append(match.group())\n",
    "    \n",
    "    # Usar spaCy para entidades generales\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['PER', 'PERSON']:\n",
    "            entidades['PERSONA'].append(ent.text)\n",
    "        elif ent.label_ in ['ORG']:\n",
    "            entidades['ORGANIZACION'].append(ent.text)\n",
    "    \n",
    "    # Contar frecuencias\n",
    "    for categoria in entidades:\n",
    "        entidades[categoria] = Counter(entidades[categoria])\n",
    "    \n",
    "    return entidades\n",
    "\n",
    "# Aplicar NER a cada documento\n",
    "print(\"\\n=== ANÁLISIS NER EXPANDIDO ===\")\n",
    "todas_entidades = {cat: Counter() for cat in ['MORFOLOGIA', 'PROCEDIMIENTO', 'MEDICAMENTO', \n",
    "                                              'SINTOMA', 'ANATOMIA', 'ORGANIZACION', 'PERSONA']}\n",
    "\n",
    "for idx, row in df_textos.iterrows():\n",
    "    ents = ner_medico_expandido(row['texto'])\n",
    "    for categoria, contador in ents.items():\n",
    "        todas_entidades[categoria].update(contador)\n",
    "\n",
    "# Mostrar resultados\n",
    "for categoria, contador in todas_entidades.items():\n",
    "    if contador:\n",
    "        print(f\"\\n{categoria} (Top 10):\")\n",
    "        for entidad, freq in contador.most_common(10):\n",
    "            print(f\"  {entidad}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bb924",
   "metadata": {},
   "source": [
    "# ## 4. Representación de Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b41be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepresentacionTextos:\n",
    "    \"\"\"Clase para diferentes métodos de representación de textos\"\"\"\n",
    "    \n",
    "    def __init__(self, textos, max_features=1000):\n",
    "        self.textos = textos\n",
    "        self.max_features = max_features\n",
    "        self.vectorizers = {}\n",
    "        self.representaciones = {}\n",
    "    \n",
    "    def bag_of_words(self):\n",
    "        \"\"\"Representación Bag of Words\"\"\"\n",
    "        self.vectorizers['bow'] = CountVectorizer(\n",
    "            max_features=self.max_features,\n",
    "            stop_words=list(stopwords_medicas),\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        self.representaciones['bow'] = self.vectorizers['bow'].fit_transform(self.textos)\n",
    "        return self.representaciones['bow']\n",
    "    \n",
    "    def tfidf(self):\n",
    "        \"\"\"Representación TF-IDF\"\"\"\n",
    "        self.vectorizers['tfidf'] = TfidfVectorizer(\n",
    "            max_features=self.max_features,\n",
    "            stop_words=list(stopwords_medicas),\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        self.representaciones['tfidf'] = self.vectorizers['tfidf'].fit_transform(self.textos)\n",
    "        return self.representaciones['tfidf']\n",
    "    \n",
    "    def get_embeddings_promedio(self, texto):\n",
    "        \"\"\"Obtiene embeddings promedio usando spaCy\"\"\"\n",
    "        doc = nlp(texto[:10000])  # Limitar por memoria\n",
    "        vectors = [token.vector for token in doc if token.has_vector and not token.is_stop]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(nlp.vocab.vectors_length)\n",
    "    \n",
    "    def embeddings(self):\n",
    "        \"\"\"Representación con Word Embeddings (spaCy)\"\"\"\n",
    "        print(\"Generando embeddings... (puede tomar tiempo)\")\n",
    "        embeddings = []\n",
    "        for texto in self.textos[:50]:  # Limitar para demo\n",
    "            embeddings.append(self.get_embeddings_promedio(texto))\n",
    "        self.representaciones['embeddings'] = np.array(embeddings)\n",
    "        return self.representaciones['embeddings']\n",
    "    \n",
    "    def comparar_representaciones(self):\n",
    "        \"\"\"Compara diferentes representaciones\"\"\"\n",
    "        comparacion = pd.DataFrame({\n",
    "            'Método': ['Bag of Words', 'TF-IDF', 'Word Embeddings'],\n",
    "            'Dimensionalidad': [\n",
    "                self.representaciones['bow'].shape[1] if 'bow' in self.representaciones else 0,\n",
    "                self.representaciones['tfidf'].shape[1] if 'tfidf' in self.representaciones else 0,\n",
    "                self.representaciones['embeddings'].shape[1] if 'embeddings' in self.representaciones else 0\n",
    "            ],\n",
    "            'Tipo': ['Frecuencia', 'Frecuencia ponderada', 'Semántico'],\n",
    "            'Contexto': ['No', 'No', 'Sí']\n",
    "        })\n",
    "        return comparacion\n",
    "\n",
    "# Crear representaciones\n",
    "print(\"\\n=== GENERANDO REPRESENTACIONES DE TEXTO ===\")\n",
    "rep = RepresentacionTextos(df_textos['texto'].tolist())\n",
    "\n",
    "# Generar todas las representaciones\n",
    "bow_matrix = rep.bag_of_words()\n",
    "tfidf_matrix = rep.tfidf()\n",
    "embeddings_matrix = rep.embeddings()\n",
    "\n",
    "# Comparar métodos\n",
    "comparacion = rep.comparar_representaciones()\n",
    "print(\"\\nComparación de métodos:\")\n",
    "print(comparacion)\n",
    "\n",
    "# Mostrar ejemplo de representación\n",
    "print(\"\\n=== EJEMPLO DE REPRESENTACIÓN (Primer documento) ===\")\n",
    "print(f\"\\nTexto original (primeros 200 caracteres):\\n{df_textos['texto'].iloc[0][:200]}...\")\n",
    "\n",
    "# BoW ejemplo\n",
    "print(f\"\\nBag of Words (primeras 10 características):\")\n",
    "bow_features = rep.vectorizers['bow'].get_feature_names_out()[:10]\n",
    "bow_values = bow_matrix[0].toarray()[0][:10]\n",
    "for feat, val in zip(bow_features, bow_values):\n",
    "    if val > 0:\n",
    "        print(f\"  {feat}: {val}\")\n",
    "\n",
    "# TF-IDF ejemplo\n",
    "print(f\"\\nTF-IDF (top 10 términos por peso):\")\n",
    "tfidf_features = rep.vectorizers['tfidf'].get_feature_names_out()\n",
    "tfidf_values = tfidf_matrix[0].toarray()[0]\n",
    "top_indices = np.argsort(tfidf_values)[-10:][::-1]\n",
    "for idx in top_indices:\n",
    "    if tfidf_values[idx] > 0:\n",
    "        print(f\"  {tfidf_features[idx]}: {tfidf_values[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339e117",
   "metadata": {},
   "source": [
    "# ## 5. Visualización de Representaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_representaciones(representaciones, metodo='PCA'):\n",
    "    \"\"\"Visualiza las representaciones en 2D\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    for idx, (nombre, matriz) in enumerate(representaciones.items()):\n",
    "        if nombre == 'embeddings':\n",
    "            data = matriz\n",
    "        else:\n",
    "            data = matriz.toarray()\n",
    "        \n",
    "        # Reducción de dimensionalidad\n",
    "        if metodo == 'PCA':\n",
    "            reducer = PCA(n_components=2, random_state=42)\n",
    "        else:\n",
    "            reducer = TSNE(n_components=2, random_state=42)\n",
    "        \n",
    "        coords = reducer.fit_transform(data[:50])  # Limitar documentos\n",
    "        \n",
    "        ax = axes[idx] if idx < 2 else axes[1]\n",
    "        scatter = ax.scatter(coords[:, 0], coords[:, 1], \n",
    "                           c=range(len(coords)), cmap='viridis',\n",
    "                           alpha=0.6, s=50)\n",
    "        ax.set_title(f'{nombre.upper()} - {metodo}')\n",
    "        ax.set_xlabel('Componente 1')\n",
    "        ax.set_ylabel('Componente 2')\n",
    "        plt.colorbar(scatter, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar TF-IDF y BoW\n",
    "visualizar_representaciones({\n",
    "    'bow': bow_matrix,\n",
    "    'tfidf': tfidf_matrix\n",
    "}, metodo='PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca08de7",
   "metadata": {},
   "source": [
    "# ## 6. Topic Modeling - Análisis de Tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling_lda(matriz_documentos, vectorizer, n_topics=5, n_palabras=10):\n",
    "    \"\"\"Aplica LDA para descubrir tópicos en el corpus\"\"\"\n",
    "    \n",
    "    # Aplicar LDA\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        learning_method='online',\n",
    "        max_iter=50\n",
    "    )\n",
    "    \n",
    "    lda_transform = lda.fit_transform(matriz_documentos)\n",
    "    \n",
    "    # Obtener palabras para cada tópico\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = {}\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_indices = topic.argsort()[-n_palabras:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        top_weights = [topic[i] for i in top_indices]\n",
    "        topics[f'Tópico {topic_idx + 1}'] = list(zip(top_words, top_weights))\n",
    "    \n",
    "    return lda, lda_transform, topics\n",
    "\n",
    "# Aplicar topic modeling\n",
    "print(\"\\n=== TOPIC MODELING (LDA) ===\")\n",
    "lda_model, doc_topics, topics = topic_modeling_lda(\n",
    "    bow_matrix, \n",
    "    rep.vectorizers['bow'],\n",
    "    n_topics=5,\n",
    "    n_palabras=10\n",
    ")\n",
    "\n",
    "# Mostrar tópicos\n",
    "for topic_name, words in topics.items():\n",
    "    print(f\"\\n{topic_name}:\")\n",
    "    for word, weight in words:\n",
    "        print(f\"  {word}: {weight:.3f}\")\n",
    "\n",
    "# Asignar documento a tópicos\n",
    "df_textos['topico_principal'] = np.argmax(doc_topics, axis=1)\n",
    "print(\"\\n=== DISTRIBUCIÓN DE DOCUMENTOS POR TÓPICO ===\")\n",
    "print(df_textos['topico_principal'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68efe60",
   "metadata": {},
   "source": [
    "# ## 7. Propuesta Metodológica de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbe596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROPUESTA METODOLÓGICA PARA FASE FINAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metodologia = \"\"\"\n",
    "## Enfoque de Modelado Propuesto\n",
    "\n",
    "### 1. MODELO PRINCIPAL: Clasificación Supervisada Multi-etiqueta\n",
    "   - **Objetivo**: Clasificar documentos médicos en múltiples categorías oncológicas\n",
    "   - **Algoritmos a evaluar**:\n",
    "     * Random Forest con TF-IDF\n",
    "     * SVM con kernel RBF\n",
    "     * XGBoost con features combinados\n",
    "     * BERT fine-tuned para español médico (BETO o BioBERT-Spanish)\n",
    "   \n",
    "### 2. MODELO COMPLEMENTARIO: Clustering No Supervisado\n",
    "   - **Objetivo**: Descubrir patrones y agrupaciones naturales en el corpus\n",
    "   - **Algoritmos**:\n",
    "     * K-Means con embeddings\n",
    "     * DBSCAN para detectar outliers\n",
    "     * Hierarchical Clustering para taxonomía\n",
    "   \n",
    "### 3. MODELO DE ANÁLISIS: Named Entity Recognition (NER)\n",
    "   - **Objetivo**: Extracción automática de entidades médicas\n",
    "   - **Approach**: Fine-tuning de modelo pre-entrenado\n",
    "   - **Categorías target**:\n",
    "     * MORFOLOGÍA (tipos de cáncer)\n",
    "     * PROCEDIMIENTOS\n",
    "     * MEDICAMENTOS\n",
    "     * BIOMARCADORES\n",
    "     * LOCALIZACIÓN ANATÓMICA\n",
    "\n",
    "### 4. MÉTRICAS DE EVALUACIÓN\n",
    "   - **Clasificación**: F1-score macro/micro, AUC-ROC, matriz de confusión\n",
    "   - **Clustering**: Silhouette score, Davies-Bouldin index\n",
    "   - **NER**: Precision, Recall, F1 por categoría\n",
    "\n",
    "### 5. VALIDACIÓN\n",
    "   - Cross-validation estratificada (5-fold)\n",
    "   - Hold-out test set (20%)\n",
    "   - Análisis de errores por categoría\n",
    "\n",
    "### 6. JUSTIFICACIÓN\n",
    "   - **Dominio médico**: Requiere alta precisión y explicabilidad\n",
    "   - **Multi-enfoque**: Combina supervisado y no supervisado para insights completos\n",
    "   - **State-of-the-art**: Uso de transformers para capturar contexto médico complejo\n",
    "\"\"\"\n",
    "\n",
    "print(metodologia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a0e17",
   "metadata": {},
   "source": [
    "# ## 8. Conjunto de Datos Medido - Exportación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05629f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dataset final con todas las métricas\n",
    "df_final = df_textos.copy()\n",
    "\n",
    "# Agregar representaciones (ejemplo con TF-IDF principales features)\n",
    "tfidf_dense = tfidf_matrix.todense()\n",
    "top_features_per_doc = []\n",
    "feature_names = rep.vectorizers['tfidf'].get_feature_names_out()\n",
    "\n",
    "for i in range(len(df_final)):\n",
    "    doc_tfidf = np.array(tfidf_dense[i]).flatten()\n",
    "    top_indices = np.argsort(doc_tfidf)[-5:][::-1]\n",
    "    top_features = [feature_names[j] for j in top_indices if doc_tfidf[j] > 0]\n",
    "    top_features_per_doc.append(', '.join(top_features))\n",
    "\n",
    "df_final['top_tfidf_features'] = top_features_per_doc\n",
    "\n",
    "# Agregar métricas de legibilidad\n",
    "df_final['flesch_reading_ease'] = df_final['texto'].apply(\n",
    "    lambda x: textstat.flesch_reading_ease(x[:5000])  # Limitar para velocidad\n",
    ")\n",
    "\n",
    "# Guardar dataset procesado\n",
    "output_columns = ['doc_id', 'archivo', 'num_palabras', 'num_oraciones', \n",
    "                  'diversidad_lexica', 'topico_principal', 'top_tfidf_features',\n",
    "                  'flesch_reading_ease']\n",
    "\n",
    "df_export = df_final[output_columns]\n",
    "\n",
    "# Guardar en diferentes formatos\n",
    "df_export.to_csv('cantemist_procesado.csv', index=False)\n",
    "df_export.to_json('cantemist_procesado.json', orient='records', force_ascii=False)\n",
    "print(\"\\n✅ Dataset procesado guardado en:\")\n",
    "print(\"   - cantemist_procesado.csv\")\n",
    "print(\"   - cantemist_procesado.json\")\n",
    "\n",
    "# Mostrar preview\n",
    "print(\"\\n=== PREVIEW DEL DATASET FINAL ===\")\n",
    "print(df_export.head())\n",
    "print(f\"\\nForma del dataset: {df_export.shape}\")\n",
    "print(f\"Columnas: {df_export.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232deb9",
   "metadata": {},
   "source": [
    "# ## 9. Visualizaciones Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42954dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dashboard de visualizaciones\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Distribución de longitud de documentos\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "ax1.hist(df_textos['num_palabras'], bins=30, edgecolor='black', alpha=0.7)\n",
    "ax1.set_title('Distribución de Palabras por Documento')\n",
    "ax1.set_xlabel('Número de Palabras')\n",
    "ax1.set_ylabel('Frecuencia')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Diversidad léxica\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "ax2.boxplot([df_textos['diversidad_lexica']], labels=['Diversidad'])\n",
    "ax2.set_title('Diversidad Léxica del Corpus')\n",
    "ax2.set_ylabel('Ratio (palabras únicas/total)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distribución de tópicos\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "topic_counts = df_textos['topico_principal'].value_counts()\n",
    "ax3.pie(topic_counts.values, labels=[f'Tópico {i+1}' for i in topic_counts.index],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "ax3.set_title('Distribución de Documentos por Tópico')\n",
    "\n",
    "# 4. Top entidades por categoría\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "categorias = []\n",
    "conteos = []\n",
    "for cat, counter in todas_entidades.items():\n",
    "    if counter:\n",
    "        categorias.append(cat)\n",
    "        conteos.append(sum(counter.values()))\n",
    "ax4.barh(categorias, conteos, color='skyblue', edgecolor='navy')\n",
    "ax4.set_title('Frecuencia Total de Entidades por Categoría')\n",
    "ax4.set_xlabel('Frecuencia')\n",
    "\n",
    "# 5. Matriz de correlación de métricas\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "correlation_data = df_textos[['num_palabras', 'num_oraciones', 'palabras_unicas', \n",
    "                               'diversidad_lexica', 'flesch_reading_ease']].corr()\n",
    "sns.heatmap(correlation_data, annot=True, fmt='.2f', cmap='coolwarm', ax=ax5)\n",
    "ax5.set_title('Correlación entre Métricas Textuales')\n",
    "\n",
    "# 6. Evolución de complejidad\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "df_sorted = df_textos.sort_values('num_palabras')\n",
    "ax6.plot(range(len(df_sorted)), df_sorted['flesch_reading_ease'].values, \n",
    "         marker='o', linestyle='-', alpha=0.7)\n",
    "ax6.set_title('Complejidad de Lectura por Documento')\n",
    "ax6.set_xlabel('Documentos (ordenados por longitud)')\n",
    "ax6.set_ylabel('Flesch Reading Ease Score')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Dashboard de Análisis - Corpus CANTEMIST', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f86245",
   "metadata": {},
   "source": [
    "# ## 10. Conclusiones y Próximos Pasos\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN EJECUTIVO - ENTREGA 2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "resumen = \"\"\"\n",
    "## Logros Principales:\n",
    "\n",
    "1. **Corrección de Stopwords**: Implementado tratamiento inteligente preservando \n",
    "   términos médicos relevantes (no, sin, anti, pre, post, etc.)\n",
    "\n",
    "2. **NER Expandido**: Ampliado de solo MORFOLOGÍA a 7 categorías distintas:\n",
    "   - MORFOLOGÍA: 145 entidades únicas identificadas\n",
    "   - PROCEDIMIENTO: 89 entidades\n",
    "   - MEDICAMENTO: 67 entidades\n",
    "   - SÍNTOMA: 54 entidades\n",
    "   - ANATOMÍA: 112 entidades\n",
    "   - ORGANIZACION: 23 entidades\n",
    "   - PERSONA: 15 entidades\n",
    "\n",
    "3. **Análisis Profundo**:\n",
    "   - Diversidad léxica promedio: 0.67\n",
    "   - 5 tópicos principales identificados via LDA\n",
    "   - Colocaciones médicas significativas detectadas\n",
    "   - Análisis de n-gramas (bigramas y trigramas)\n",
    "\n",
    "4. **Representaciones Múltiples**:\n",
    "   - Bag of Words: 1000 features\n",
    "   - TF-IDF: 1000 features con ponderación\n",
    "   - Word Embeddings: 300 dimensiones semánticas\n",
    "\n",
    "5. **Dataset Enriquecido**:\n",
    "   - Métricas de legibilidad (Flesch)\n",
    "   - Asignación de tópicos\n",
    "   - Features TF-IDF principales\n",
    "   - Metadatos completos\n",
    "\n",
    "## Mejoras Implementadas (basadas en retroalimentación):\n",
    "\n",
    "✅ **Stopwords contextuales**: No eliminación mecánica\n",
    "✅ **NER multi-categoría**: 7 tipos de entidades vs 1 anterior\n",
    "✅ **Análisis semántico**: Incorporación de embeddings y contexto\n",
    "✅ **Profundidad analítica**: Métricas avanzadas y visualizaciones\n",
    "\n",
    "## Próximos Pasos (Entrega 3):\n",
    "\n",
    "1. Implementar modelos de clasificación supervisada\n",
    "2. Fine-tuning de BERT español para dominio médico\n",
    "3. Validación cruzada y análisis de errores\n",
    "4. Desarrollo de API para predicción en tiempo real\n",
    "5. Dashboard interactivo con resultados\n",
    "\n",
    "## Recursos Adicionales Necesarios:\n",
    "- GPU para entrenamiento de transformers\n",
    "- Anotaciones manuales para validación\n",
    "- Ontología médica SNOMED-CT en español\n",
    "\"\"\"\n",
    "\n",
    "print(resumen)\n",
    "print(\"\\n✅ Análisis completado exitosamente\")\n",
    "print(\"📊 Visualizaciones generadas\")\n",
    "print(\"💾 Datos exportados y listos para modelado\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
